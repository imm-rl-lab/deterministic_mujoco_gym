{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "domain_name: cheetah\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 0.01312239 -0.00935882 -0.04109939  0.09649481  0.14012104  0.1683507\n",
      "  0.02070408  0.16647046  0.11370937  0.54693767 -0.4472241  -1.88019192\n",
      "  4.69290717  6.17567871  7.56102531 -0.13626821  8.17694774  5.54283914]\n",
      "reward: 0.003025796887300758\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 0.05969411266716018\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 277.0788251132033\n",
      "reward difference with virual step: 3.0\n",
      "state difference with virual step from the middle: 56.49291572989734\n",
      "reward difference with virual step from the middle: 3.0\n",
      "time: 1.4091212749481201\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from DMControl.DMControlEnv import DMControlEnv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def env_test(domain_name, task_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('domain_name:', domain_name)\n",
    "    print('task_name:', task_name)\n",
    "\n",
    "    env = DMControlEnv(domain_name, task_name, dt=0.03)\n",
    "\n",
    "    print('state_dim:', env.state_dim)\n",
    "    print('action_dim:', env.action_dim)\n",
    "    print('action_min:', env.action_min)\n",
    "    print('action_max:', env.action_max)\n",
    "    print('step_limit:', env.env._step_limit)\n",
    "    \n",
    "    state = env.reset()\n",
    "    print('initial_state:', state)\n",
    "    \n",
    "    fixed_action = np.ones(env.action_dim)\n",
    "    next_state, reward, done, _ = env.step(fixed_action)\n",
    "    print('next_state:', next_state)\n",
    "    print('reward:', reward)\n",
    "    print('done:', done)\n",
    "    print('real dt:', env.env.physics.data.time)\n",
    "    \n",
    "    def get_session(state, episode_n, action, step_type='step'):\n",
    "        states, rewards = [state], []\n",
    "        for _ in range(episode_n):\n",
    "            if step_type == 'step':\n",
    "                state, reward, done, _ = env.step(action)\n",
    "            elif step_type == 'virtual_step':\n",
    "                state, reward, done, _ = env.virtual_step(state, action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return states, rewards\n",
    "    \n",
    "    episode_n = int(env.inner_step_limit / env.inner_step_n) - 1\n",
    "    print('episode_n:', episode_n)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    states, rewards = get_session(initial_state, episode_n, \n",
    "                                  fixed_action, step_type='step')\n",
    "    \n",
    "    print('total_reward:', sum(rewards))\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    new_states, new_rewards = get_session(initial_state, episode_n, \n",
    "                                          fixed_action, step_type='step')\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(new_states), axis=1))\n",
    "    print('state difference in two attempt:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(new_rewards)))\n",
    "    print('reward difference in two attempt:', reward_diff)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    virt_states, virt_rewards = get_session(initial_state, episode_n, \n",
    "                                            fixed_action, step_type='virtual_step')\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(virt_states), axis=1))\n",
    "    print('state difference with virual step:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(virt_rewards)))\n",
    "    print('reward difference with virual step:', reward_diff)\n",
    "    \n",
    "    mid_episode = int(episode_n / 2) \n",
    "    mid_virt_states, mid_virt_rewards = get_session(states[mid_episode], episode_n - mid_episode, \n",
    "                                                    fixed_action, step_type='virtual_step')\n",
    "\n",
    "    state_diff = np.max(np.linalg.norm(np.array(states[mid_episode:]) - np.array(mid_virt_states), axis=1))\n",
    "    print('state difference with virual step from the middle:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards[mid_episode:]) - np.array(mid_virt_rewards)))\n",
    "    print('reward difference with virual step from the middle:', reward_diff)    \n",
    "    print('time:', time.time() - start_time)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "domain_name, task_name = 'cheetah', 'run'\n",
    "env_test(domain_name, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: acrobot\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00353233  0.01124544 -0.23549702  0.74971488]\n",
      "reward: 3.0\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 126.44781464280857\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.8120222091674805\n",
      "\n",
      "\n",
      "domain_name: acrobot\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00353233  0.01124544 -0.23549702  0.74971488]\n",
      "reward: 3.0\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 29.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.7440025806427002\n",
      "\n",
      "\n",
      "domain_name: ball_in_cup\n",
      "task_name: catch\n",
      "state_dim: 8\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [0.21657576 0.17477124 0.10433546 0.18792739 0.21740194 0.22312317\n",
      " 0.65506705 0.04221043]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.3000000000000002\n",
      "episode_n: 65\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.011128134948676219\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.012030349885079488\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.5419893264770508\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00438039 -0.00636909  0.29204711 -0.42508881]\n",
      "reward: 2.3897063100725213\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 135.3821659545606\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 3.177644019200215e-14\n",
      "reward difference with virual step: 4.829470157119431e-15\n",
      "state difference with virual step from the middle: 3.177644019200215e-14\n",
      "reward difference with virual step from the middle: 4.829470157119431e-15\n",
      "time: 1.1525983810424805\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00438039 -0.00636909  0.29204711 -0.42508881]\n",
      "reward: 3.0\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 3.177644019200215e-14\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 3.177644019200215e-14\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.8229794502258301\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00438039 -0.00636909  0.29204711 -0.42508881]\n",
      "reward: 2.3897063100725213\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 135.3821659545606\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 3.177644019200215e-14\n",
      "reward difference with virual step: 4.829470157119431e-15\n",
      "state difference with virual step from the middle: 3.177644019200215e-14\n",
      "reward difference with virual step from the middle: 4.829470157119431e-15\n",
      "time: 1.136000394821167\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00438039 -0.00636909  0.29204711 -0.42508881]\n",
      "reward: 3.0\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 3.177644019200215e-14\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 3.177644019200215e-14\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.8160617351531982\n",
      "\n",
      "\n",
      "domain_name: cheetah\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 0.01312239 -0.00935882 -0.04109939  0.09649481  0.14012104  0.1683507\n",
      "  0.02070408  0.16647046  0.11370937  0.54693767 -0.4472241  -1.88019192\n",
      "  4.69290717  6.17567871  7.56102531 -0.13626821  8.17694774  5.54283914]\n",
      "reward: 0.003025796887300758\n",
      "done: False\n",
      "real dt: 0.03\n",
      "episode_n: 332\n",
      "total_reward: 0.05969411266716018\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 277.0788251132033\n",
      "reward difference with virual step: 3.0\n",
      "state difference with virual step from the middle: 56.49291572989734\n",
      "reward difference with virual step from the middle: 3.0\n",
      "time: 1.4100003242492676\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: spin\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.14486586  0.25311127  0.         10.06026455  5.19819971  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.060000000000000005\n",
      "episode_n: 332\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 17.04578112270614\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 1.1224486213085985\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.8300001621246338\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: turn_easy\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.14486586  0.25311127  0.         10.06026455  5.19819971  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.060000000000000005\n",
      "episode_n: 332\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 16.80745489263356\n",
      "reward difference with virual step: 3.0\n",
      "state difference with virual step from the middle: 1.122448621308595\n",
      "reward difference with virual step from the middle: 3.0\n",
      "time: 1.1629936695098877\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: turn_hard\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.14486586  0.25311127  0.         10.06026455  5.19819971  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.060000000000000005\n",
      "episode_n: 332\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 16.80745489263356\n",
      "reward difference with virual step: 3.0\n",
      "state difference with virual step from the middle: 1.122448621308595\n",
      "reward difference with virual step from the middle: 3.0\n",
      "time: 1.1769776344299316\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: upright\n",
      "Warning: the closest possible dt is  0.028\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-0.00304879 -0.00205179  0.10593019  0.99674708  0.01494418 -0.03770679\n",
      " -0.06964289  0.98494928  0.54986153 -0.41769186  0.80738099  0.684536\n",
      " -0.69121656  0.67344755 -0.0094442  -0.0101744   0.02433941  0.06945647\n",
      " -0.3333638  -0.20445682  0.78970382  1.6029603   1.2682134   1.88569349\n",
      "  1.54761267 -1.78975563  1.56303447]\n",
      "reward: 6.999955209990034\n",
      "done: False\n",
      "real dt: 0.2800000000000002\n",
      "episode_n: 141\n",
      "total_reward: 758.4853757529866\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 2.0490352277875252\n",
      "reward difference with virual step: 6.255513281261961\n",
      "state difference with virual step from the middle: 2.0704933860354937\n",
      "reward difference with virual step from the middle: 4.7329729926994695\n",
      "time: 0.6760225296020508\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: swim\n",
      "Warning: the closest possible dt is  0.028\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-0.00304879 -0.00205179  0.10593019  0.99674708  0.01494418 -0.03770679\n",
      " -0.06964289  0.98494928  0.54986153 -0.41769186  0.80738099  0.684536\n",
      " -0.69121656  0.67344755 -0.0094442  -0.0101744   0.02433941  0.06945647\n",
      " -0.3333638  -0.20445682  0.78970382  1.6029603   1.2682134   1.88569349\n",
      "  1.54761267 -1.78975563  1.56303447]\n",
      "reward: 0.8744621416257916\n",
      "done: False\n",
      "real dt: 0.2800000000000002\n",
      "episode_n: 141\n",
      "total_reward: 103.68676674655428\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0028249148050197848\n",
      "state difference with virual step: 2.0490352277875252\n",
      "reward difference with virual step: 2.2157325219636164\n",
      "state difference with virual step from the middle: 2.0704933860354937\n",
      "reward difference with virual step from the middle: 3.9556833479934994\n",
      "time: 0.8659813404083252\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: stand\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 4.25852228e-03 -4.92388207e-02 -8.83321807e-01  5.97190111e-01\n",
      "  1.86712987e-01  6.68263500e-01 -3.42588911e-02 -2.78393210e-01\n",
      " -7.45043010e-01 -5.86482351e+00  1.59273283e+00 -2.62463629e-01\n",
      "  1.08507996e+01 -1.91966498e+00]\n",
      "reward: 4.8\n",
      "done: False\n",
      "real dt: 0.12000000000000004\n",
      "episode_n: 165\n",
      "total_reward: 8.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 1.8788776908613387\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.14201921385588823\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.8420226573944092\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: hop\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 4.25852228e-03 -4.92388207e-02 -8.83321807e-01  5.97190111e-01\n",
      "  1.86712987e-01  6.68263500e-01 -3.42588911e-02 -2.78393210e-01\n",
      " -7.45043010e-01 -5.86482351e+00  1.59273283e+00 -2.62463629e-01\n",
      "  1.08507996e+01 -1.91966498e+00]\n",
      "reward: 0.26649577915617806\n",
      "done: False\n",
      "real dt: 0.12000000000000004\n",
      "episode_n: 165\n",
      "total_reward: 0.6969458459827517\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 1.8788776908613387\n",
      "reward difference with virual step: 0.00932022378493702\n",
      "state difference with virual step from the middle: 0.14201921385588823\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.7280054092407227\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: stand\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [ 1.62768581e-03 -5.37665208e-03  1.34329956e+00  9.03559663e-01\n",
      " -3.47419784e-02 -3.20525311e-01 -2.82199320e-01  8.11423565e-01\n",
      "  6.01849401e-01  5.60883906e-01  7.99532995e-02  6.45176933e-01\n",
      "  3.58302135e-01  4.78122685e-02  8.94782700e-01  8.88999025e-01\n",
      "  8.21619936e-02  6.87551788e-01  3.56144150e-01  4.10288660e-02\n",
      "  8.96171873e-01  8.94730781e-01  1.07031322e+00  1.10874129e+00\n",
      "  9.03267141e-01  1.54301763e+00  1.50187736e+00  8.86014307e-01\n",
      " -2.16920204e-01  1.06665398e-01 -1.78431553e+00 -4.07563257e-01\n",
      " -2.46108521e+00  2.29926661e+00  2.98961627e+00  6.34155728e-01\n",
      "  7.09003931e+00 -1.07890509e+00  1.73346319e+00  5.55498689e-01\n",
      "  1.14061196e+00 -9.03023763e-01 -2.14453751e-01 -1.27411486e+00\n",
      "  2.22969458e+00 -6.30915258e-01 -4.87048366e-01 -4.44620541e-01\n",
      " -7.32721047e-01 -8.58783418e-01 -1.91826754e+00 -1.33914385e+00\n",
      " -2.31371571e+00 -5.50320320e-01 -3.33502228e-01]\n",
      "reward: 4.707326071295508\n",
      "done: False\n",
      "real dt: 0.15000000000000005\n",
      "episode_n: 165\n",
      "total_reward: 15.130377471378248\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 25.3960642322476\n",
      "reward difference with virual step: 1.3707587167219142\n",
      "state difference with virual step from the middle: 4.964095975785028\n",
      "reward difference with virual step from the middle: 2.7500789944298975e-09\n",
      "time: 1.8550488948822021\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: walk\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [ 1.62768581e-03 -5.37665208e-03  1.34329956e+00  9.03559663e-01\n",
      " -3.47419784e-02 -3.20525311e-01 -2.82199320e-01  8.11423565e-01\n",
      "  6.01849401e-01  5.60883906e-01  7.99532995e-02  6.45176933e-01\n",
      "  3.58302135e-01  4.78122685e-02  8.94782700e-01  8.88999025e-01\n",
      "  8.21619936e-02  6.87551788e-01  3.56144150e-01  4.10288660e-02\n",
      "  8.96171873e-01  8.94730781e-01  1.07031322e+00  1.10874129e+00\n",
      "  9.03267141e-01  1.54301763e+00  1.50187736e+00  8.86014307e-01\n",
      " -2.16920204e-01  1.06665398e-01 -1.78431553e+00 -4.07563257e-01\n",
      " -2.46108521e+00  2.29926661e+00  2.98961627e+00  6.34155728e-01\n",
      "  7.09003931e+00 -1.07890509e+00  1.73346319e+00  5.55498689e-01\n",
      "  1.14061196e+00 -9.03023763e-01 -2.14453751e-01 -1.27411486e+00\n",
      "  2.22969458e+00 -6.30915258e-01 -4.87048366e-01 -4.44620541e-01\n",
      " -7.32721047e-01 -8.58783418e-01 -1.91826754e+00 -1.33914385e+00\n",
      " -2.31371571e+00 -5.50320320e-01 -3.33502228e-01]\n",
      "reward: 1.329245252972994\n",
      "done: False\n",
      "real dt: 0.15000000000000005\n",
      "episode_n: 165\n",
      "total_reward: 8.338878302983355\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 25.3960642322476\n",
      "reward difference with virual step: 1.9048547180389066\n",
      "state difference with virual step from the middle: 4.964095975785028\n",
      "reward difference with virual step from the middle: 6.819943785556982e-10\n",
      "time: 1.8300001621246338\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: run\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [ 1.62768581e-03 -5.37665208e-03  1.34329956e+00  9.03559663e-01\n",
      " -3.47419784e-02 -3.20525311e-01 -2.82199320e-01  8.11423565e-01\n",
      "  6.01849401e-01  5.60883906e-01  7.99532995e-02  6.45176933e-01\n",
      "  3.58302135e-01  4.78122685e-02  8.94782700e-01  8.88999025e-01\n",
      "  8.21619936e-02  6.87551788e-01  3.56144150e-01  4.10288660e-02\n",
      "  8.96171873e-01  8.94730781e-01  1.07031322e+00  1.10874129e+00\n",
      "  9.03267141e-01  1.54301763e+00  1.50187736e+00  8.86014307e-01\n",
      " -2.16920204e-01  1.06665398e-01 -1.78431553e+00 -4.07563257e-01\n",
      " -2.46108521e+00  2.29926661e+00  2.98961627e+00  6.34155728e-01\n",
      "  7.09003931e+00 -1.07890509e+00  1.73346319e+00  5.55498689e-01\n",
      "  1.14061196e+00 -9.03023763e-01 -2.14453751e-01 -1.27411486e+00\n",
      "  2.22969458e+00 -6.30915258e-01 -4.87048366e-01 -4.44620541e-01\n",
      " -7.32721047e-01 -8.58783418e-01 -1.91826754e+00 -1.33914385e+00\n",
      " -2.31371571e+00 -5.50320320e-01 -3.33502228e-01]\n",
      "reward: 0.8434807081490526\n",
      "done: False\n",
      "real dt: 0.15000000000000005\n",
      "episode_n: 165\n",
      "total_reward: 3.3031327108432693\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 25.3960642322476\n",
      "reward difference with virual step: 0.4795001873168998\n",
      "state difference with virual step from the middle: 4.964095975785028\n",
      "reward difference with virual step from the middle: 4.733368624419634e-10\n",
      "time: 1.8285908699035645\n",
      "\n",
      "\n",
      "domain_name: manipulator\n",
      "task_name: bring_ball\n",
      "state_dim: 22\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.4 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0. ]\n",
      "next_state: [ 1.82143599e+00  1.54765836e+00  1.12994797e+00  1.15250170e+00\n",
      "  5.98285030e-01 -1.18115876e-04  5.99092914e-01  9.61605696e-05\n",
      "  3.71688192e-01  2.17736165e-02 -1.40569107e+00  6.39002453e+00\n",
      "  5.27878063e+00  3.80216599e+00  3.87466706e+00  1.98242148e+00\n",
      "  1.33661356e-04  2.00813188e+00 -1.12272510e-04 -1.18075462e+00\n",
      "  2.85256291e-02 -5.40558078e+01]\n",
      "reward: 1.6063723234775724e-166\n",
      "done: False\n",
      "real dt: 0.3000000000000002\n",
      "episode_n: 32\n",
      "total_reward: 1.0558827568183498e-57\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 2.3265502804436688e-58\n",
      "state difference with virual step: 1.2188528554422071\n",
      "reward difference with virual step: 2.843635571482669e-36\n",
      "state difference with virual step from the middle: 1.2359817879798283\n",
      "reward difference with virual step from the middle: 1.0550861792448049e-12\n",
      "time: 1.4940001964569092\n",
      "\n",
      "\n",
      "domain_name: pendulum\n",
      "task_name: swingup\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 2\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0.]\n",
      "next_state: [0.00158103 0.07905138]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 66.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 612.2968262794979\n",
      "reward difference with virual step: 1.0\n",
      "state difference with virual step from the middle: 204.66202323117378\n",
      "reward difference with virual step from the middle: 1.0\n",
      "time: 1.1390094757080078\n",
      "\n",
      "\n",
      "domain_name: point_mass\n",
      "task_name: easy\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [0.000125 0.000125 0.00625  0.00625 ]\n",
      "reward: 0.8\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 14.748656503394045\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 88.33220042327116\n",
      "reward difference with virual step: 0.4495720958905468\n",
      "state difference with virual step from the middle: 22.578033088048315\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.5229957103729248\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: easy\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 1.0\n",
      "state difference with virual step: 27819.962344870626\n",
      "reward difference with virual step: 1.0\n",
      "state difference with virual step from the middle: 7077.373467238399\n",
      "reward difference with virual step from the middle: 1.0\n",
      "time: 1.3780090808868408\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: hard\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 27819.962344870626\n",
      "reward difference with virual step: 1.0\n",
      "state difference with virual step from the middle: 7077.373467238399\n",
      "reward difference with virual step from the middle: 1.0\n",
      "time: 1.3770198822021484\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer6\n",
      "state_dim: 16\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.04018207  0.03182882 -0.74911779  1.07111128 -0.35197825  0.06070559\n",
      " -0.35405532  1.07099557 -0.05281883  0.12414194 -1.55517665  2.156899\n",
      " -0.43660238 -0.32748664 -0.44479926  2.1572486 ]\n",
      "reward: 0.507272166587292\n",
      "done: False\n",
      "real dt: 0.45000000000000034\n",
      "episode_n: 65\n",
      "total_reward: 7.320534495453387\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.33244937816231107\n",
      "state difference with virual step: 0.578434451170379\n",
      "reward difference with virual step: 12.059625257612685\n",
      "state difference with virual step from the middle: 0.16008218854102904\n",
      "reward difference with virual step from the middle: 14.88528001980531\n",
      "time: 0.796067476272583\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer15\n",
      "state_dim: 34\n",
      "action_dim: 14\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-3.34098035e-02  1.08601062e-02 -4.59626939e-01  4.84434114e-01\n",
      "  7.79773390e-02 -1.22627237e-01  2.16084901e-02 -1.74885051e-03\n",
      " -4.48784775e-05  2.80049748e-05  2.76162705e-05 -4.38997173e-05\n",
      " -1.74160192e-03  2.15032522e-02 -1.22058508e-01  7.68269664e-02\n",
      "  4.84428469e-01 -5.98476919e-02  3.27474689e-02 -6.23442591e-01\n",
      "  4.88420105e-02  1.21271859e+00 -7.43249126e-01  1.07842255e-01\n",
      " -7.76207768e-04 -2.32410938e-03  3.89401098e-04  3.84554342e-04\n",
      " -2.28514267e-03 -9.71252356e-04  1.08383615e-01 -7.43379057e-01\n",
      "  1.21104722e+00  4.89487964e-02]\n",
      "reward: 0.09335565164065887\n",
      "done: False\n",
      "real dt: 0.45000000000000034\n",
      "episode_n: 65\n",
      "total_reward: 12.2333515222738\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.9640217588411051\n",
      "state difference with virual step: 1.4912663574710185\n",
      "reward difference with virual step: 14.238580303711286\n",
      "state difference with virual step from the middle: 0.2774387348988164\n",
      "reward difference with virual step from the middle: 11.438422728892238\n",
      "time: 1.1789720058441162\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: stand\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2750695   0.01243953  1.36067217  1.7542704   0.00887993  0.79448894\n",
      "  1.7542704   0.00887993  0.79448894 -0.01164699 -0.39901513 -0.38381356\n",
      "  0.00549984  0.00587299 -0.01391406  0.00549984  0.00587299 -0.01391406]\n",
      "reward: 9.867294260313615\n",
      "done: False\n",
      "real dt: 0.3000000000000002\n",
      "episode_n: 82\n",
      "total_reward: 269.4841091306807\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.7837095745273717\n",
      "reward difference with virual step: 0.6322456067632904\n",
      "state difference with virual step from the middle: 0.22931713072636448\n",
      "reward difference with virual step from the middle: 0.00037033507676875743\n",
      "time: 0.8110249042510986\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: walk\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2750695   0.01243953  1.36067217  1.7542704   0.00887993  0.79448894\n",
      "  1.7542704   0.00887993  0.79448894 -0.01164699 -0.39901513 -0.38381356\n",
      "  0.00549984  0.00587299 -0.01391406  0.00549984  0.00587299 -0.01391406]\n",
      "reward: 1.6445490433856025\n",
      "done: False\n",
      "real dt: 0.3000000000000002\n",
      "episode_n: 82\n",
      "total_reward: 45.01806540516108\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.7837095745273717\n",
      "reward difference with virual step: 0.10537426779388182\n",
      "state difference with virual step from the middle: 0.22931713072636448\n",
      "reward difference with virual step from the middle: 6.172251279501495e-05\n",
      "time: 0.8899810314178467\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2750695   0.01243953  1.36067217  1.7542704   0.00887993  0.79448894\n",
      "  1.7542704   0.00887993  0.79448894 -0.01164699 -0.39901513 -0.38381356\n",
      "  0.00549984  0.00587299 -0.01391406  0.00549984  0.00587299 -0.01391406]\n",
      "reward: 1.6445490433856025\n",
      "done: False\n",
      "real dt: 0.3000000000000002\n",
      "episode_n: 82\n",
      "total_reward: 44.927024090536094\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.7837095745273717\n",
      "reward difference with virual step: 0.10537426779388182\n",
      "state difference with virual step from the middle: 0.22931713072636448\n",
      "reward difference with virual step from the middle: 6.172251279468188e-05\n",
      "time: 0.9091360569000244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dm_control import suite\n",
    "\n",
    "max_len = max(len(d) for d, _ in suite.BENCHMARKING)\n",
    "for domain, task in suite.BENCHMARKING:\n",
    "    env_test(domain, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
