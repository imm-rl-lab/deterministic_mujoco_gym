{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "domain_name: pendulum\n",
      "task_name: swingup\n",
      "state_dim: 2\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0.]\n",
      "next_state: [0.00158103 0.07905138]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 23.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.6696059703826904\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from DMControlEnv.DMControlEnvWithPhysics import DMControlEnvWithPhysics\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def env_test(domain_name, task_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('domain_name:', domain_name)\n",
    "    print('task_name:', task_name)\n",
    "\n",
    "    env = DMControlEnvWithPhysics(domain_name, task_name)\n",
    "\n",
    "    print('state_dim:', env.state_dim)\n",
    "    print('action_dim:', env.action_dim)\n",
    "    print('action_min:', env.action_min)\n",
    "    print('action_max:', env.action_max)\n",
    "    \n",
    "    state = env.reset()\n",
    "    print('initial_state:', state)\n",
    "    \n",
    "    fixed_action = np.ones(env.action_dim)\n",
    "    next_state, reward, done, _ = env.step(fixed_action)\n",
    "    print('next_state:', next_state)\n",
    "    print('reward:', reward)\n",
    "    print('done:', done)\n",
    "    print('dt:', env.env.physics.data.time)\n",
    "    \n",
    "    def get_session(initial_state, episode_n, action):\n",
    "        states, rewards = [initial_state], []\n",
    "        for _ in range(episode_n):\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "        return states, rewards\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    states, rewards = get_session(initial_state, 200, fixed_action)\n",
    "    print('total_reward:', sum(rewards))\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    new_states, new_rewards = get_session(initial_state, 200, fixed_action)\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(new_states), axis=1))\n",
    "    print('state difference in two attempt:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(new_rewards)))\n",
    "    print('reward difference in two attempt:', reward_diff)\n",
    "    \n",
    "    state = env.reset()\n",
    "    virt_states, virt_rewards = [state], []\n",
    "    for _ in range(200):\n",
    "        state, reward, done, _ = env.virtual_step(state, fixed_action)\n",
    "        virt_states.append(state)\n",
    "        virt_rewards.append(reward)\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(virt_states), axis=1))\n",
    "    print('state difference with virual step:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(virt_rewards)))\n",
    "    print('reward difference with virual step:', reward_diff)\n",
    "    \n",
    "    state = states[100]\n",
    "    mid_virt_states, mid_virt_rewards = [state], []\n",
    "    for _ in range(100):\n",
    "        state, reward, done, _ = env.virtual_step(state, fixed_action)\n",
    "        mid_virt_states.append(state)\n",
    "        mid_virt_rewards.append(reward)\n",
    "\n",
    "    state_diff = np.max(np.linalg.norm(np.array(states[100:]) - np.array(mid_virt_states), axis=1))\n",
    "    print('state difference with virual step from the middle:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards[100:]) - np.array(mid_virt_rewards)))\n",
    "    print('reward difference with virual step from the middle:', reward_diff)    \n",
    "    print('time:', time.time() - start_time)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "domain_name, task_name = 'pendulum', 'swingup'\n",
    "env_test(domain_name, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: acrobot\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00039307  0.00125134 -0.07855389  0.25007942]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 49.90536711616123\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.8524985313415527\n",
      "\n",
      "\n",
      "domain_name: acrobot\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00039307  0.00125134 -0.07855389  0.25007942]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 20.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.607025146484375\n",
      "\n",
      "\n",
      "domain_name: ball_in_cup\n",
      "task_name: catch\n",
      "state_dim: 8\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 1.19969748e-02  9.79498269e-03  2.26260791e-05 -1.38989304e-03\n",
      "  9.50661585e-01  6.12862205e-01  7.68380608e-03  5.65788800e-02]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.020000000000000004\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.6703765392303467\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 0.7992638140705812\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 39.083294516809794\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.3822176456451416\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1717417240142822\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 0.7992638140705812\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 39.083294516809794\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2717132568359375\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1936335563659668\n",
      "\n",
      "\n",
      "domain_name: cheetah\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 3.47039647e-03 -1.94397198e-03 -8.32927969e-03  1.80151493e-02\n",
      "  2.96646892e-02  3.36916859e-02  2.01279059e-02  2.33202088e-02\n",
      "  1.86159766e-02  3.47039647e-01 -1.94397198e-01 -8.32927969e-01\n",
      "  1.80151493e+00  2.96646892e+00  3.36916859e+00  2.01279059e+00\n",
      "  2.33202088e+00  1.86159766e+00]\n",
      "reward: 0.00020665777830264354\n",
      "done: False\n",
      "dt: 0.01\n",
      "total_reward: 0.05969411266716018\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1246376037597656\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: spin\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.49134488  0.06242939  0.          5.17256006  3.59395966  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1172685623168945\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: turn_easy\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.49134488  0.06242939  0.          5.17256006  3.59395966  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2780389785766602\n",
      "\n",
      "\n",
      "domain_name: finger\n",
      "task_name: turn_hard\n",
      "state_dim: 6\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [-1.57079633  0.          0.          0.          0.          0.        ]\n",
      "next_state: [-1.49134488  0.06242939  0.          5.17256006  3.59395966  0.        ]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1857657432556152\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: upright\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-2.54448483e-04 -5.21973821e-05  1.00295668e-01  9.99935068e-01\n",
      "  1.13185310e-03 -2.84851312e-03 -1.09756826e-02  2.69720236e-01\n",
      "  6.34521220e-02 -2.89196535e-01  1.09195079e-01  1.20657342e-01\n",
      " -9.58207904e-02  1.17302662e-01 -9.56997739e-03 -2.63607840e-03\n",
      "  1.22680306e-02  1.04650018e-01 -2.10738612e-01 -7.51520407e-01\n",
      "  6.72799763e+00  1.97452160e+00 -5.58787972e+00  3.27934276e+00\n",
      "  3.15718922e+00 -2.75699556e+00  3.03981227e+00]\n",
      "reward: 0.9999999991870193\n",
      "done: False\n",
      "dt: 0.04000000000000001\n",
      "total_reward: 193.23438350392001\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.3093445301055908\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: swim\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-2.54448483e-04 -5.21973821e-05  1.00295668e-01  9.99935068e-01\n",
      "  1.13185310e-03 -2.84851312e-03 -1.09756826e-02  2.69720236e-01\n",
      "  6.34521220e-02 -2.89196535e-01  1.09195079e-01  1.20657342e-01\n",
      " -9.58207904e-02  1.17302662e-01 -9.56997739e-03 -2.63607840e-03\n",
      "  1.22680306e-02  1.04650018e-01 -2.10738612e-01 -7.51520407e-01\n",
      "  6.72799763e+00  1.97452160e+00 -5.58787972e+00  3.27934276e+00\n",
      "  3.15718922e+00 -2.75699556e+00  3.03981227e+00]\n",
      "reward: 0.12499882561024254\n",
      "done: False\n",
      "dt: 0.04000000000000001\n",
      "total_reward: 23.650562214788447\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 1.9135355754776318e-05\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 1.899825396703414e-05\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 1.899825396703414e-05\n",
      "time: 1.2688286304473877\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: stand\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 2.40104168e-04 -2.34987327e-03 -3.88460036e-02  1.81650060e-02\n",
      "  2.24253663e-02  3.55101928e-02  8.98984720e-03  2.49670844e-02\n",
      " -1.91358433e-01 -3.07171904e+00  1.48909898e+00  1.87413160e+00\n",
      "  2.39867796e+00  7.41352602e-01]\n",
      "reward: 0.8\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 7.999999999999999\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2237393856048584\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: hop\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 2.40104168e-04 -2.34987327e-03 -3.88460036e-02  1.81650060e-02\n",
      "  2.24253663e-02  3.55101928e-02  8.98984720e-03  2.49670844e-02\n",
      " -1.91358433e-01 -3.07171904e+00  1.48909898e+00  1.87413160e+00\n",
      "  2.39867796e+00  7.41352602e-01]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.6969458459827517\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2022912502288818\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: stand\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-1.70420615e-03 -3.70646479e-03  1.49491199e+00  9.96813621e-01\n",
      " -6.78837074e-03  1.56542537e-02 -7.79196154e-02  1.51920982e-01\n",
      " -1.45317289e-01  4.71075812e-02 -1.72215365e-02  1.42118339e-01\n",
      "  1.91461346e-01  9.44753364e-02  2.67018743e-01  3.89415067e-01\n",
      "  7.12188073e-02  1.76605735e-01  1.92794027e-01  9.77254604e-02\n",
      "  3.08051257e-01  5.09164304e-01  1.54131565e-01  1.84531608e-01\n",
      "  1.14873517e-01  1.83546094e-01  2.32096094e-01  3.41081543e-01\n",
      " -9.01128095e-02 -2.23284534e-01 -3.93474684e-01 -8.93864371e-01\n",
      "  1.63522691e+00 -9.47517806e+00  8.35895395e+00 -7.75224911e+00\n",
      "  2.67901851e+00 -8.04214865e-03  7.61705925e+00  8.25545249e+00\n",
      " -7.09558620e-01  1.32897077e+01  2.55659837e+01  4.90904465e+00\n",
      "  8.49436023e+00  8.71460789e+00 -6.02105725e-01  1.63634379e+01\n",
      "  3.08493436e+01  1.01890574e+01  1.08524102e+01  8.07732582e+00\n",
      "  1.13016758e+01  1.47170680e+01  2.13535231e+01]\n",
      "reward: 0.7999014297047755\n",
      "done: False\n",
      "dt: 0.025\n",
      "total_reward: 15.130377380113842\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.577883005142212\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: walk\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-1.70420615e-03 -3.70646479e-03  1.49491199e+00  9.96813621e-01\n",
      " -6.78837074e-03  1.56542537e-02 -7.79196154e-02  1.51920982e-01\n",
      " -1.45317289e-01  4.71075812e-02 -1.72215365e-02  1.42118339e-01\n",
      "  1.91461346e-01  9.44753364e-02  2.67018743e-01  3.89415067e-01\n",
      "  7.12188073e-02  1.76605735e-01  1.92794027e-01  9.77254604e-02\n",
      "  3.08051257e-01  5.09164304e-01  1.54131565e-01  1.84531608e-01\n",
      "  1.14873517e-01  1.83546094e-01  2.32096094e-01  3.41081543e-01\n",
      " -9.01128095e-02 -2.23284534e-01 -3.93474684e-01 -8.93864371e-01\n",
      "  1.63522691e+00 -9.47517806e+00  8.35895395e+00 -7.75224911e+00\n",
      "  2.67901851e+00 -8.04214865e-03  7.61705925e+00  8.25545249e+00\n",
      " -7.09558620e-01  1.32897077e+01  2.55659837e+01  4.90904465e+00\n",
      "  8.49436023e+00  8.71460789e+00 -6.02105725e-01  1.63634379e+01\n",
      "  3.08493436e+01  1.01890574e+01  1.08524102e+01  8.07732582e+00\n",
      "  1.13016758e+01  1.47170680e+01  2.13535231e+01]\n",
      "reward: 0.14712760081064624\n",
      "done: False\n",
      "dt: 0.025\n",
      "total_reward: 8.338878287740677\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.551177978515625\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: run\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-1.70420615e-03 -3.70646479e-03  1.49491199e+00  9.96813621e-01\n",
      " -6.78837074e-03  1.56542537e-02 -7.79196154e-02  1.51920982e-01\n",
      " -1.45317289e-01  4.71075812e-02 -1.72215365e-02  1.42118339e-01\n",
      "  1.91461346e-01  9.44753364e-02  2.67018743e-01  3.89415067e-01\n",
      "  7.12188073e-02  1.76605735e-01  1.92794027e-01  9.77254604e-02\n",
      "  3.08051257e-01  5.09164304e-01  1.54131565e-01  1.84531608e-01\n",
      "  1.14873517e-01  1.83546094e-01  2.32096094e-01  3.41081543e-01\n",
      " -9.01128095e-02 -2.23284534e-01 -3.93474684e-01 -8.93864371e-01\n",
      "  1.63522691e+00 -9.47517806e+00  8.35895395e+00 -7.75224911e+00\n",
      "  2.67901851e+00 -8.04214865e-03  7.61705925e+00  8.25545249e+00\n",
      " -7.09558620e-01  1.32897077e+01  2.55659837e+01  4.90904465e+00\n",
      "  8.49436023e+00  8.71460789e+00 -6.02105725e-01  1.63634379e+01\n",
      "  3.08493436e+01  1.01890574e+01  1.08524102e+01  8.07732582e+00\n",
      "  1.13016758e+01  1.47170680e+01  2.13535231e+01]\n",
      "reward: 0.13471276008106467\n",
      "done: False\n",
      "dt: 0.025\n",
      "total_reward: 3.3031326956293365\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.6977481842041016\n",
      "\n",
      "\n",
      "domain_name: manipulator\n",
      "task_name: bring_ball\n",
      "state_dim: 22\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "initial_state: [0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.4 0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0. ]\n",
      "next_state: [ 1.25803675e-02  1.98875978e-02  1.99810744e-02  3.06758427e-02\n",
      "  2.44192737e-02 -1.02358905e-04  1.54830604e-02  7.29635206e-05\n",
      "  4.00000000e-01  3.99460450e-01  0.00000000e+00  2.47859676e+00\n",
      "  2.93613070e+00  2.62788954e+00  3.39605822e+00  2.45242305e+00\n",
      " -3.84183070e-03  1.54464261e+00  3.87070339e-03  0.00000000e+00\n",
      " -9.81000000e-02  0.00000000e+00]\n",
      "reward: 4.856895201850267e-269\n",
      "done: False\n",
      "dt: 0.010000000000000002\n",
      "total_reward: 6.673381224765572e-59\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 3.0764612985268407e-59\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 4.504334070968535e-29\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 4.504334070968535e-29\n",
      "time: 1.4417366981506348\n",
      "\n",
      "\n",
      "domain_name: pendulum\n",
      "task_name: swingup\n",
      "state_dim: 2\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "initial_state: [0. 0.]\n",
      "next_state: [0.00158103 0.07905138]\n",
      "reward: 1.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 23.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.040703296661377\n",
      "\n",
      "\n",
      "domain_name: point_mass\n",
      "task_name: easy\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [0.000125 0.000125 0.00625  0.00625 ]\n",
      "reward: 0.8\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 14.748656503394045\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1507782936096191\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: easy\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 1.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2250804901123047\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: hard\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "dt: 0.02\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.1055755615234375\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer6\n",
      "state_dim: 16\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-8.98845959e-04  1.09588915e-05 -1.42170212e-02  2.02603968e-02\n",
      " -6.99943401e-03  1.91211980e-03 -6.99944128e-03  2.02603908e-02\n",
      " -5.46158000e-02  1.27938022e-03 -8.63995584e-01  1.23218199e+00\n",
      " -4.27495915e-01  1.18619592e-01 -4.27497262e-01  1.23218089e+00]\n",
      "reward: 0.03576954468800127\n",
      "done: False\n",
      "dt: 0.030000000000000013\n",
      "total_reward: 1.388855938430865\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.023503796992519255\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.006942998449654798\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0066539991120143655\n",
      "time: 1.249072551727295\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer15\n",
      "state_dim: 34\n",
      "action_dim: 14\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-8.98883110e-04  1.09546510e-05 -1.42182358e-02  2.02614901e-02\n",
      " -6.92811643e-03  9.56426622e-04 -7.16906968e-05 -8.67442503e-07\n",
      "  1.17080494e-06 -1.77247579e-07 -1.77248409e-07  1.17080319e-06\n",
      " -8.67372680e-07 -7.16914438e-05  9.56431068e-04 -6.92812443e-03\n",
      "  2.02614842e-02 -5.46178476e-02  1.27884798e-03 -8.64063047e-01\n",
      "  1.23221761e+00 -4.22960640e-01  5.93536464e-02 -4.57579976e-03\n",
      " -3.13066608e-05  7.04453410e-05 -1.09046749e-05 -1.09048269e-05\n",
      "  7.04450131e-05 -3.12937097e-05 -4.57593816e-03  5.93544693e-02\n",
      " -4.22962123e-01  1.23221652e+00]\n",
      "reward: 0.0062512208784216575\n",
      "done: False\n",
      "dt: 0.030000000000000013\n",
      "total_reward: 2.4888188917202063\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.006365024061120033\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0037257659691275168\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.002686503528647499\n",
      "time: 1.40574049949646\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: stand\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-1.54500786e-03  2.82659244e-03  1.23462876e-01  2.21096580e-01\n",
      " -1.18510001e-01  8.60213641e-02  2.21096580e-01 -1.18510001e-01\n",
      "  8.60213641e-02 -2.95926971e-01  2.07969816e-01  8.86059266e+00\n",
      "  1.57553303e+01 -8.16417821e+00  6.84538308e+00  1.57553303e+01\n",
      " -8.16417821e+00  6.84538308e+00]\n",
      "reward: 0.9990485169461739\n",
      "done: False\n",
      "dt: 0.024999999999999998\n",
      "total_reward: 68.262333447409\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.4395010471343994\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: walk\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-1.54500786e-03  2.82659244e-03  1.23462876e-01  2.21096580e-01\n",
      " -1.18510001e-01  8.60213641e-02  2.21096580e-01 -1.18510001e-01\n",
      "  8.60213641e-02 -2.95926971e-01  2.07969816e-01  8.86059266e+00\n",
      "  1.57553303e+01 -8.16417821e+00  6.84538308e+00  1.57553303e+01\n",
      " -8.16417821e+00  6.84538308e+00]\n",
      "reward: 0.16650808615769566\n",
      "done: False\n",
      "dt: 0.024999999999999998\n",
      "total_reward: 11.481102791282519\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2815461158752441\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-1.54500786e-03  2.82659244e-03  1.23462876e-01  2.21096580e-01\n",
      " -1.18510001e-01  8.60213641e-02  2.21096580e-01 -1.18510001e-01\n",
      "  8.60213641e-02 -2.95926971e-01  2.07969816e-01  8.86059266e+00\n",
      "  1.57553303e+01 -8.16417821e+00  6.84538308e+00  1.57553303e+01\n",
      " -8.16417821e+00  6.84538308e+00]\n",
      "reward: 0.16650808615769566\n",
      "done: False\n",
      "dt: 0.024999999999999998\n",
      "total_reward: 11.390061476657555\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.2730722427368164\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dm_control import suite\n",
    "\n",
    "max_len = max(len(d) for d, _ in suite.BENCHMARKING)\n",
    "for domain, task in suite.BENCHMARKING:\n",
    "    env_test(domain, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
