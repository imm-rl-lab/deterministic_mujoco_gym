{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: cheetah\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 3.47039647e-03 -1.94397198e-03 -8.32927969e-03  1.80151493e-02\n",
      "  2.96646892e-02  3.36916859e-02  2.01279059e-02  2.33202088e-02\n",
      "  1.86159766e-02  3.47039647e-01 -1.94397198e-01 -8.32927969e-01\n",
      "  1.80151493e+00  2.96646892e+00  3.36916859e+00  2.01279059e+00\n",
      "  2.33202088e+00  1.86159766e+00]\n",
      "reward: 0.00020665777830264354\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 0.05969411266716018\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 5.058534860610962\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from DMControl.DMControlEnvWithPhysics import DMControlEnvWithPhysics\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def env_test(domain_name, task_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('domain_name:', domain_name)\n",
    "    print('task_name:', task_name)\n",
    "\n",
    "    env = DMControlEnvWithPhysics(domain_name, task_name, dt=0.01)\n",
    "\n",
    "    print('state_dim:', env.state_dim)\n",
    "    print('action_dim:', env.action_dim)\n",
    "    print('action_min:', env.action_min)\n",
    "    print('action_max:', env.action_max)\n",
    "    print('step_limit:', env.env._step_limit)\n",
    "    \n",
    "    state = env.reset()\n",
    "    print('initial_state:', state)\n",
    "    \n",
    "    fixed_action = np.ones(env.action_dim)\n",
    "    next_state, reward, done, _ = env.step(fixed_action)\n",
    "    print('next_state:', next_state)\n",
    "    print('reward:', reward)\n",
    "    print('done:', done)\n",
    "    print('real dt:', env.env.physics.data.time)\n",
    "    \n",
    "    def get_session(state, episode_n, action, step_type='step'):\n",
    "        states, rewards = [state], []\n",
    "        for t in range(episode_n):\n",
    "            if step_type == 'step':\n",
    "                state, reward, done, _ = env.step(action)\n",
    "            elif step_type == 'virtual_step':\n",
    "                state, reward, done, _ = env.virtual_step(state, action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return states, rewards\n",
    "    \n",
    "    episode_n = int(env.inner_step_limit / env.inner_step_n) - 1\n",
    "    print('episode_n:', episode_n)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    states, rewards = get_session(initial_state, episode_n, \n",
    "                                  fixed_action, step_type='step')\n",
    "    \n",
    "    print('total_reward:', sum(rewards))\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    new_states, new_rewards = get_session(initial_state, episode_n, \n",
    "                                          fixed_action, step_type='step')\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(new_states), axis=1))\n",
    "    print('state difference in two attempt:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(new_rewards)))\n",
    "    print('reward difference in two attempt:', reward_diff)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    virt_states, virt_rewards = get_session(initial_state, episode_n, \n",
    "                                            fixed_action, step_type='virtual_step')\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(virt_states), axis=1))\n",
    "    print('state difference with virual step:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(virt_rewards)))\n",
    "    print('reward difference with virual step:', reward_diff)\n",
    "    \n",
    "    mid_episode = int(episode_n / 2) \n",
    "    mid_virt_states, mid_virt_rewards = get_session(states[mid_episode], episode_n - mid_episode, \n",
    "                                                    fixed_action, step_type='virtual_step')\n",
    "\n",
    "    state_diff = np.max(np.linalg.norm(np.array(states[mid_episode:]) - np.array(mid_virt_states), axis=1))\n",
    "    print('state difference with virual step from the middle:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards[mid_episode:]) - np.array(mid_virt_rewards)))\n",
    "    print('reward difference with virual step from the middle:', reward_diff)    \n",
    "    print('time:', time.time() - start_time)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "domain_name, task_name = 'cheetah', 'run'\n",
    "env_test(domain_name, task_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: acrobot\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00039307  0.00125134 -0.07855389  0.25007942]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 126.44781467174717\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 2.9801032543182373\n",
      "\n",
      "\n",
      "domain_name: acrobot\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00039307  0.00125134 -0.07855389  0.25007942]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 29.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 2.8320181369781494\n",
      "\n",
      "\n",
      "domain_name: ball_in_cup\n",
      "task_name: catch\n",
      "state_dim: 8\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [0.10643276 0.06344381 0.01101376 0.06818863 1.0431486  0.8178902\n",
      " 0.26901945 1.12009794]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.10000000000000007\n",
      "episode_n: 199\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.9949991703033447\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 0.7992638140705812\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 136.00804098924203\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.2830774784088135\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: balance_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 2.9535598754882812\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 0.7992638140705812\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 136.00804098924203\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.297532081604004\n",
      "\n",
      "\n",
      "domain_name: cartpole\n",
      "task_name: swingup_sparse\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [ 0.00048668 -0.00070696  0.0973366  -0.14141007]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 11.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 2.9680280685424805\n",
      "\n",
      "\n",
      "domain_name: cheetah\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 3.47039647e-03 -1.94397198e-03 -8.32927969e-03  1.80151493e-02\n",
      "  2.96646892e-02  3.36916859e-02  2.01279059e-02  2.33202088e-02\n",
      "  1.86159766e-02  3.47039647e-01 -1.94397198e-01 -8.32927969e-01\n",
      "  1.80151493e+00  2.96646892e+00  3.36916859e+00  2.01279059e+00\n",
      "  2.33202088e+00  1.86159766e+00]\n",
      "reward: 0.00020665777830264354\n",
      "done: False\n",
      "real dt: 0.01\n",
      "episode_n: 999\n",
      "total_reward: 0.05969411266716018\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 5.0031821727752686\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: upright\n",
      "Warning: the closest possible dt is  0.008\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-7.18050482e-04 -2.18105792e-04  1.00964965e-01  9.99620471e-01\n",
      "  3.82895606e-03 -7.70494941e-03 -2.61703282e-02  5.04285790e-01\n",
      "  1.46660802e-01 -4.43502683e-01  2.44724927e-01  2.41826904e-01\n",
      " -2.07106331e-01  2.34293116e-01 -1.26522601e-02 -5.34434933e-03\n",
      "  1.98179412e-02  1.52064706e-01 -2.57536209e-01 -7.28648511e-01\n",
      "  5.14901334e+00  2.15301475e+00 -2.70377090e+00  3.42821139e+00\n",
      "  2.91621385e+00 -2.79640557e+00  2.83032520e+00]\n",
      "reward: 1.9999999487141809\n",
      "done: False\n",
      "real dt: 0.08000000000000004\n",
      "episode_n: 499\n",
      "total_reward: 764.9234396944031\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.9030849933624268\n",
      "\n",
      "\n",
      "domain_name: fish\n",
      "task_name: swim\n",
      "Warning: the closest possible dt is  0.008\n",
      "state_dim: 27\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  0.1 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "next_state: [-7.18050482e-04 -2.18105792e-04  1.00964965e-01  9.99620471e-01\n",
      "  3.82895606e-03 -7.70494941e-03 -2.61703282e-02  5.04285790e-01\n",
      "  1.46660802e-01 -4.43502683e-01  2.44724927e-01  2.41826904e-01\n",
      " -2.07106331e-01  2.34293116e-01 -1.26522601e-02 -5.34434933e-03\n",
      "  1.98179412e-02  1.52064706e-01 -2.57536209e-01 -7.28648511e-01\n",
      "  5.14901334e+00  2.15301475e+00 -2.70377090e+00  3.42821139e+00\n",
      "  2.91621385e+00 -2.79640557e+00  2.83032520e+00]\n",
      "reward: 0.24998957221656315\n",
      "done: False\n",
      "real dt: 0.08000000000000004\n",
      "episode_n: 499\n",
      "total_reward: 104.73466795021204\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0008236234160664824\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 1.7495095237270581\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 1.7419292487441074\n",
      "time: 2.153560161590576\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: stand\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 1.19616717e-03 -8.90074077e-03 -1.37842750e-01  6.73285002e-02\n",
      "  8.50275943e-02  1.03529788e-01  3.38414506e-02  6.17494118e-02\n",
      " -4.14653275e-01 -6.07494335e+00  3.03849418e+00  3.88039913e+00\n",
      "  4.00083532e+00  1.54906182e+00]\n",
      "reward: 1.6\n",
      "done: False\n",
      "real dt: 0.04\n",
      "episode_n: 499\n",
      "total_reward: 8.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.9920260906219482\n",
      "\n",
      "\n",
      "domain_name: hopper\n",
      "task_name: hop\n",
      "state_dim: 14\n",
      "action_dim: 4\n",
      "action_min: [-1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [ 1.19616717e-03 -8.90074077e-03 -1.37842750e-01  6.73285002e-02\n",
      "  8.50275943e-02  1.03529788e-01  3.38414506e-02  6.17494118e-02\n",
      " -4.14653275e-01 -6.07494335e+00  3.03849418e+00  3.88039913e+00\n",
      "  4.00083532e+00  1.54906182e+00]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.04\n",
      "episode_n: 499\n",
      "total_reward: 0.6969458459827517\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.8660240173339844\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: stand\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-2.53121636e-04 -7.94356551e-03  1.47682610e+00  9.74595778e-01\n",
      " -2.30281262e-02  1.55361357e-02 -2.22241767e-01  3.88406235e-01\n",
      " -2.89308820e-01  7.52518043e-02  1.22133651e-01  3.43722676e-01\n",
      "  4.13988008e-01  7.17987095e-02  8.24394151e-01  1.02906870e+00\n",
      "  1.44920637e-01  2.94374382e-01  4.32286200e-01  6.24926859e-02\n",
      "  9.86005900e-01  1.17977493e+00  5.72055241e-01  4.70444727e-01\n",
      "  4.76874120e-01  5.96231561e-01  7.49406530e-01  9.87913685e-01\n",
      "  1.77054663e-01 -1.25662867e-01 -7.93483961e-01 -7.66097260e-01\n",
      " -2.89037062e+00 -1.08698854e+01  8.79117239e+00 -2.26230485e-01\n",
      "  1.10757147e+00  8.28581511e+00  7.62842049e+00  6.94057936e+00\n",
      " -2.19419820e-01  2.84161939e+01  8.81151253e+00  6.31871794e-01\n",
      "  2.82952125e+00  6.99806098e+00 -5.78694566e-01  3.38961455e+01\n",
      "  6.43858390e+00  2.09756201e+01  8.85141589e+00  1.95576772e+01\n",
      "  2.02604022e+01  2.25836702e+01  1.59492771e+01]\n",
      "reward: 1.596881951094678\n",
      "done: False\n",
      "real dt: 0.049999999999999996\n",
      "episode_n: 499\n",
      "total_reward: 15.1303774723026\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.451342821121216\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: walk\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-2.53121636e-04 -7.94356551e-03  1.47682610e+00  9.74595778e-01\n",
      " -2.30281262e-02  1.55361357e-02 -2.22241767e-01  3.88406235e-01\n",
      " -2.89308820e-01  7.52518043e-02  1.22133651e-01  3.43722676e-01\n",
      "  4.13988008e-01  7.17987095e-02  8.24394151e-01  1.02906870e+00\n",
      "  1.44920637e-01  2.94374382e-01  4.32286200e-01  6.24926859e-02\n",
      "  9.86005900e-01  1.17977493e+00  5.72055241e-01  4.70444727e-01\n",
      "  4.76874120e-01  5.96231561e-01  7.49406530e-01  9.87913685e-01\n",
      "  1.77054663e-01 -1.25662867e-01 -7.93483961e-01 -7.66097260e-01\n",
      " -2.89037062e+00 -1.08698854e+01  8.79117239e+00 -2.26230485e-01\n",
      "  1.10757147e+00  8.28581511e+00  7.62842049e+00  6.94057936e+00\n",
      " -2.19419820e-01  2.84161939e+01  8.81151253e+00  6.31871794e-01\n",
      "  2.82952125e+00  6.99806098e+00 -5.78694566e-01  3.38961455e+01\n",
      "  6.43858390e+00  2.09756201e+01  8.85141589e+00  1.95576772e+01\n",
      "  2.02604022e+01  2.25836702e+01  1.59492771e+01]\n",
      "reward: 0.3569309997897875\n",
      "done: False\n",
      "real dt: 0.049999999999999996\n",
      "episode_n: 499\n",
      "total_reward: 8.338878303137426\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.494843006134033\n",
      "\n",
      "\n",
      "domain_name: humanoid\n",
      "task_name: run\n",
      "state_dim: 55\n",
      "action_dim: 21\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0.  0.  1.5 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0. ]\n",
      "next_state: [-2.53121636e-04 -7.94356551e-03  1.47682610e+00  9.74595778e-01\n",
      " -2.30281262e-02  1.55361357e-02 -2.22241767e-01  3.88406235e-01\n",
      " -2.89308820e-01  7.52518043e-02  1.22133651e-01  3.43722676e-01\n",
      "  4.13988008e-01  7.17987095e-02  8.24394151e-01  1.02906870e+00\n",
      "  1.44920637e-01  2.94374382e-01  4.32286200e-01  6.24926859e-02\n",
      "  9.86005900e-01  1.17977493e+00  5.72055241e-01  4.70444727e-01\n",
      "  4.76874120e-01  5.96231561e-01  7.49406530e-01  9.87913685e-01\n",
      "  1.77054663e-01 -1.25662867e-01 -7.93483961e-01 -7.66097260e-01\n",
      " -2.89037062e+00 -1.08698854e+01  8.79117239e+00 -2.26230485e-01\n",
      "  1.10757147e+00  8.28581511e+00  7.62842049e+00  6.94057936e+00\n",
      " -2.19419820e-01  2.84161939e+01  8.81151253e+00  6.31871794e-01\n",
      "  2.82952125e+00  6.99806098e+00 -5.78694566e-01  3.38961455e+01\n",
      "  6.43858390e+00  2.09756201e+01  8.85141589e+00  1.95576772e+01\n",
      "  2.02604022e+01  2.25836702e+01  1.59492771e+01]\n",
      "reward: 0.27569309997897873\n",
      "done: False\n",
      "real dt: 0.049999999999999996\n",
      "episode_n: 499\n",
      "total_reward: 3.303132710997336\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.470301628112793\n",
      "\n",
      "\n",
      "domain_name: pendulum\n",
      "task_name: swingup\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 2\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0.]\n",
      "next_state: [0.00158103 0.07905138]\n",
      "reward: 1.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 66.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 2.665285348892212\n",
      "\n",
      "\n",
      "domain_name: point_mass\n",
      "task_name: easy\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [0.000125 0.000125 0.00625  0.00625 ]\n",
      "reward: 0.8\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 14.748656503394045\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 3.122589588165283\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: easy\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 1.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 1.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 1.0\n",
      "time: 2.8730454444885254\n",
      "\n",
      "\n",
      "domain_name: reacher\n",
      "task_name: hard\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0.]\n",
      "next_state: [-0.00324371  0.0553179  -0.16218539  2.76589489]\n",
      "reward: 0.0\n",
      "done: False\n",
      "real dt: 0.02\n",
      "episode_n: 999\n",
      "total_reward: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 1.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 1.0\n",
      "time: 2.929001569747925\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer6\n",
      "state_dim: 16\n",
      "action_dim: 5\n",
      "action_min: [-1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-1.33883728e-02  2.68421289e-03 -2.13849629e-01  3.06920441e-01\n",
      " -1.10500883e-01  3.48896813e-02 -1.10577572e-01  3.06894731e-01\n",
      " -1.19069339e-01  4.85534623e-02 -1.94129973e+00  2.79587512e+00\n",
      " -1.02479540e+00  3.41237900e-01 -1.02691615e+00  2.79537129e+00]\n",
      "reward: 0.17673961547537048\n",
      "done: False\n",
      "real dt: 0.1500000000000001\n",
      "episode_n: 199\n",
      "total_reward: 7.473163850714051\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.11562229509391027\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 4.96181377311675\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 4.961826131963497\n",
      "time: 1.2790212631225586\n",
      "\n",
      "\n",
      "domain_name: swimmer\n",
      "task_name: swimmer15\n",
      "state_dim: 34\n",
      "action_dim: 14\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-1.33888636e-02  2.68171020e-03 -2.13859167e-01  3.06892489e-01\n",
      " -1.09077701e-01  1.75932417e-02 -1.58350348e-03  1.88904836e-05\n",
      "  1.88153673e-05 -3.06329390e-06 -3.07167394e-06  1.87975503e-05\n",
      "  1.96082016e-05 -1.59106116e-03  1.76358239e-02 -1.09159813e-01\n",
      "  3.06864450e-01 -1.19078506e-01  4.85073549e-02 -1.94143406e+00\n",
      "  2.79588072e+00 -1.01278030e+00  1.73467832e-01 -1.52000375e-02\n",
      " -1.35754276e-04  2.32557679e-04 -3.09222788e-05 -3.11620799e-05\n",
      "  2.32187627e-04 -1.16730399e-04 -1.54048208e-02  1.74612173e-01\n",
      " -1.01502454e+00  2.79527649e+00]\n",
      "reward: 0.031188947977510467\n",
      "done: False\n",
      "real dt: 0.1500000000000001\n",
      "episode_n: 199\n",
      "total_reward: 12.51556760405266\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.3260641686010412\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 4.939630985676297\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 4.931595856435189\n",
      "time: 1.7186155319213867\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: stand\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2115812   0.0330817   1.36800758  1.89823962 -0.21136449  0.93919394\n",
      "  1.89823962 -0.21136449  0.93919394 -3.06504428  0.71191148 16.80818056\n",
      "  6.20648027 27.37591655 -3.18077753  6.20648027 27.37591655 -3.18077753]\n",
      "reward: 3.7850664720869105\n",
      "done: False\n",
      "real dt: 0.10000000000000005\n",
      "episode_n: 249\n",
      "total_reward: 272.56403426868906\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.4441330432891846\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: walk\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2115812   0.0330817   1.36800758  1.89823962 -0.21136449  0.93919394\n",
      "  1.89823962 -0.21136449  0.93919394 -3.06504428  0.71191148 16.80818056\n",
      "  6.20648027 27.37591655 -3.18077753  6.20648027 27.37591655 -3.18077753]\n",
      "reward: 0.630844412014485\n",
      "done: False\n",
      "real dt: 0.10000000000000005\n",
      "episode_n: 249\n",
      "total_reward: 45.531386261496124\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.52402925491333\n",
      "\n",
      "\n",
      "domain_name: walker\n",
      "task_name: run\n",
      "state_dim: 18\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "step_limit: 1000.0\n",
      "initial_state: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "next_state: [-0.2115812   0.0330817   1.36800758  1.89823962 -0.21136449  0.93919394\n",
      "  1.89823962 -0.21136449  0.93919394 -3.06504428  0.71191148 16.80818056\n",
      "  6.20648027 27.37591655 -3.18077753  6.20648027 27.37591655 -3.18077753]\n",
      "reward: 0.630844412014485\n",
      "done: False\n",
      "real dt: 0.10000000000000005\n",
      "episode_n: 249\n",
      "total_reward: 45.44034494687116\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 1.5130064487457275\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dm_control import suite\n",
    "\n",
    "max_len = max(len(d) for d, _ in suite.BENCHMARKING)\n",
    "for domain, task in suite.BENCHMARKING:\n",
    "    if domain not in ['finger', 'manipulator']:\n",
    "        env_test(domain, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
