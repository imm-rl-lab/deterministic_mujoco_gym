{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "domain_name: HalfCheetah-v3\n",
      "state_dim: 17\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "real dt: 0.03\n",
      "reward: 0.11267473480315826\n",
      "frame_skip: 3\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 1.2104698750217736e-13\n",
      "reward difference with virual step: 2.7755575615628914e-15\n",
      "state difference with virual step from the middle: 2.009841808917283e-14\n",
      "reward difference with virual step from the middle: 2.7755575615628914e-15\n",
      "time: 0.537001371383667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from Gym.GymEnv import GymEnv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def env_test(domain_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('domain_name:', domain_name)\n",
    "\n",
    "    env = GymEnv(domain_name, dt=0.03)\n",
    "    \n",
    "    print('state_dim:', env.state_dim)\n",
    "    print('action_dim:', env.action_dim)\n",
    "    print('action_min:', env.action_min)\n",
    "    print('action_max:', env.action_max)\n",
    "    \n",
    "    state = env.reset()\n",
    "    #print('initial_state:', state)\n",
    "    #print(state.physics)\n",
    "    \n",
    "    fixed_action = np.ones(env.action_dim)\n",
    "    next_state, reward, done, _ = env.step(fixed_action)\n",
    "    print('real dt:', env.env.data.time)\n",
    "    print('reward:', reward)\n",
    "    print('frame_skip:', env.env.env.frame_skip)\n",
    "    print('max_episode_steps', env.max_episode_steps)\n",
    "    \n",
    "\n",
    "#    print('dt:', env.env.physics.data.time)\n",
    "    \n",
    "    def get_session(state, episode_n, action, step_type='step'):\n",
    "        states, rewards = [state], []\n",
    "        for _ in range(episode_n):\n",
    "            if step_type == 'step':\n",
    "                state, reward, done, _ = env.step(action)\n",
    "            elif step_type == 'virtual_step':\n",
    "                state, reward, done, _ = env.virtual_step(state, action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        return states, rewards\n",
    "    \n",
    "    episode_n = env.max_episode_steps - 1\n",
    "    print('episode_n:', episode_n)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    print('initial state difference:', np.linalg.norm(np.array(initial_state) - np.array(state)))\n",
    "    states, rewards = get_session(initial_state, episode_n, \n",
    "                                  fixed_action, step_type='step')\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    new_states, new_rewards = get_session(initial_state, episode_n, \n",
    "                                                  fixed_action, step_type='step')\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(new_states), axis=1))\n",
    "    print('state difference in two attempt:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(new_rewards)))\n",
    "    print('reward difference in two attempt:', reward_diff)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    virt_states, virt_rewards = get_session(initial_state, episode_n, \n",
    "                                            fixed_action, step_type='virtual_step')    \n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(virt_states), axis=1))\n",
    "    print('state difference with virual step:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(virt_rewards)))\n",
    "    print('reward difference with virual step:', reward_diff)\n",
    "    \n",
    "    \n",
    "    mid_episode = int(len(states) / 2) \n",
    "    mid_virt_states, mid_virt_rewards = get_session(states[mid_episode], episode_n - mid_episode, \n",
    "                                                    fixed_action, step_type='virtual_step')\n",
    "\n",
    "    state_diff = np.max(np.linalg.norm(np.array(states[mid_episode:]) - np.array(mid_virt_states), axis=1))\n",
    "    print('state difference with virual step from the middle:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards[mid_episode:]) - np.array(mid_virt_rewards)))\n",
    "    print('reward difference with virual step from the middle:', reward_diff)    \n",
    "    print('time:', time.time() - start_time)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "domain_name = 'HalfCheetah-v3'\n",
    "env_test(domain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: Ant-v3\n",
      "state_dim: 111\n",
      "action_dim: 8\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "real dt: 0.03\n",
      "reward: -2.9979921613949783\n",
      "frame_skip: 3\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 4.4515916823129694e-13\n",
      "reward difference with virual step: 1.6969911525951282\n",
      "state difference with virual step from the middle: 2.6501101057926703e-15\n",
      "reward difference with virual step from the middle: 1.6882391951417497\n",
      "time: 1.681488037109375\n",
      "\n",
      "\n",
      "domain_name: HalfCheetah-v3\n",
      "state_dim: 17\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "real dt: 0.03\n",
      "reward: 0.11267473480315826\n",
      "frame_skip: 3\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 1.2104698750217736e-13\n",
      "reward difference with virual step: 2.7755575615628914e-15\n",
      "state difference with virual step from the middle: 2.009841808917283e-14\n",
      "reward difference with virual step from the middle: 2.7755575615628914e-15\n",
      "time: 0.5340147018432617\n",
      "\n",
      "\n",
      "domain_name: Hopper-v3\n",
      "state_dim: 11\n",
      "action_dim: 3\n",
      "action_min: [-1. -1. -1.]\n",
      "action_max: [1. 1. 1.]\n",
      "real dt: 0.030000000000000013\n",
      "reward: 1.062800126604805\n",
      "frame_skip: 15\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 5.095979762253383e-16\n",
      "reward difference with virual step: 8.881784197001252e-16\n",
      "state difference with virual step from the middle: 1.0742359164551517e-17\n",
      "reward difference with virual step from the middle: 8.881784197001252e-16\n",
      "time: 0.024011611938476562\n",
      "\n",
      "\n",
      "domain_name: Humanoid-v3\n",
      "state_dim: 376\n",
      "action_dim: 17\n",
      "action_min: [-0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4\n",
      " -0.4 -0.4 -0.4]\n",
      "action_max: [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "real dt: 0.029999999999999995\n",
      "reward: 4.727999300978742\n",
      "frame_skip: 10\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0119962524719576\n",
      "reward difference with virual step: 13.47804742279795\n",
      "state difference with virual step from the middle: 0.012177947858521544\n",
      "reward difference with virual step from the middle: 13.47804940954613\n",
      "time: 0.15100789070129395\n",
      "\n",
      "\n",
      "domain_name: HumanoidStandup-v2\n",
      "state_dim: 376\n",
      "action_dim: 17\n",
      "action_min: [-0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4\n",
      " -0.4 -0.4 -0.4]\n",
      "action_max: [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "real dt: 0.029999999999999995\n",
      "reward: 37.593507597303706\n",
      "frame_skip: 10\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 195.5551409742709\n",
      "state difference in two attempt: 816.8945434423979\n",
      "reward difference in two attempt: 2.1404541675222966\n",
      "state difference with virual step: 433.9237928404244\n",
      "reward difference with virual step: 2.4687003362390243\n",
      "state difference with virual step from the middle: 0.09555755871618789\n",
      "reward difference with virual step from the middle: 6.839729209673351e-05\n",
      "time: 5.42008113861084\n",
      "\n",
      "\n",
      "domain_name: InvertedDoublePendulum-v2\n",
      "state_dim: 11\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "real dt: 0.03\n",
      "reward: 9.281395200455185\n",
      "frame_skip: 3\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.0159757137298584\n",
      "\n",
      "\n",
      "domain_name: InvertedPendulum-v2\n",
      "Warning: the closest possible dt is  0.02\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-3.]\n",
      "action_max: [3.]\n",
      "real dt: 0.02\n",
      "reward: 1.0\n",
      "frame_skip: 1\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.013228178024291992\n",
      "\n",
      "\n",
      "domain_name: Reacher-v2\n",
      "state_dim: 11\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "real dt: 0.03\n",
      "reward: -2.328849493335258\n",
      "frame_skip: 3\n",
      "max_episode_steps 50\n",
      "episode_n: 49\n",
      "initial state difference: 0.2174912486417929\n",
      "state difference in two attempt: 6.201337298753301\n",
      "reward difference in two attempt: 0.1041767621468086\n",
      "state difference with virual step: 0.691313485838934\n",
      "reward difference with virual step: 0.2564983765625932\n",
      "state difference with virual step from the middle: 8.865046825963507e-18\n",
      "reward difference with virual step from the middle: 0.19992966473695617\n",
      "time: 0.04098916053771973\n",
      "\n",
      "\n",
      "domain_name: Swimmer-v3\n",
      "state_dim: 8\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "real dt: 0.03\n",
      "reward: 0.0005966088805260916\n",
      "frame_skip: 3\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 7.622565885970975e-15\n",
      "reward difference with virual step: 3.700815695562021e-15\n",
      "state difference with virual step from the middle: 1.1572012039102202e-17\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.6200635433197021\n",
      "\n",
      "\n",
      "domain_name: Walker2d-v3\n",
      "state_dim: 17\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "real dt: 0.030000000000000013\n",
      "reward: 1.2223681230683565\n",
      "frame_skip: 15\n",
      "max_episode_steps 1000\n",
      "episode_n: 999\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 1.1245799833639328e-14\n",
      "reward difference with virual step: 7.327471962526033e-15\n",
      "state difference with virual step from the middle: 2.626403050785775e-15\n",
      "reward difference with virual step from the middle: 3.9968028886505635e-15\n",
      "time: 0.14701080322265625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for domain_name in ['Ant-v3', 'HalfCheetah-v3', 'Hopper-v3', 'Humanoid-v3', \n",
    "                    'HumanoidStandup-v2', 'InvertedDoublePendulum-v2', 'InvertedPendulum-v2',\n",
    "                    'Reacher-v2', 'Swimmer-v3', 'Walker2d-v3']:\n",
    "    env_test(domain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
