{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "domain_name: InvertedPendulum-v2\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-3.]\n",
      "action_max: [3.]\n",
      "reward: 1.0\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.012012004852294922\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from Gym.GymEnv import GymEnv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def env_test(domain_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('domain_name:', domain_name)\n",
    "\n",
    "    env = GymEnv(domain_name)\n",
    "\n",
    "    print('state_dim:', env.state_dim)\n",
    "    print('action_dim:', env.action_dim)\n",
    "    print('action_min:', env.action_min)\n",
    "    print('action_max:', env.action_max)\n",
    "    \n",
    "    state = env.reset()\n",
    "    #print('initial_state:', state)\n",
    "    #print(state.physics)\n",
    "    \n",
    "    fixed_action = np.ones(env.action_dim)\n",
    "    next_state, reward, done, _ = env.step(fixed_action)\n",
    "    #print('next_state:', next_state)\n",
    "    print('reward:', reward)\n",
    "    print('done:', done)\n",
    "    \n",
    "    \n",
    "\n",
    "#    print('dt:', env.env.physics.data.time)\n",
    "    \n",
    "    def get_session(initial_state, episode_n, action):\n",
    "        states, rewards = [initial_state], []\n",
    "        for t in range(episode_n):\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                #print('done:', done)\n",
    "                break\n",
    "        return states, rewards, t\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    print('initial state difference:', np.linalg.norm(np.array(initial_state) - np.array(state)))\n",
    "    states, rewards, done_t = get_session(initial_state, 200, fixed_action)\n",
    "    #print('total_reward:', sum(rewards))\n",
    "    #print('done time:', done_t)\n",
    "    \n",
    "    initial_state = env.reset()\n",
    "    new_states, new_rewards, done_t = get_session(initial_state, 200, fixed_action)\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(new_states), axis=1))\n",
    "    print('state difference in two attempt:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(new_rewards)))\n",
    "    print('reward difference in two attempt:', reward_diff)\n",
    "    \n",
    "    state = env.reset()\n",
    "    virt_states, virt_rewards = [state], []\n",
    "    for _ in range(200):\n",
    "        state, reward, done, _ = env.virtual_step(state, fixed_action)\n",
    "        virt_states.append(state)\n",
    "        virt_rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    state_diff = np.max(np.linalg.norm(np.array(states) - np.array(virt_states), axis=1))\n",
    "    print('state difference with virual step:', state_diff)\n",
    "    reward_diff = np.max(np.abs(np.array(rewards) - np.array(virt_rewards)))\n",
    "    print('reward difference with virual step:', reward_diff)\n",
    "    \n",
    "    start_t = int(done_t/2) + 1\n",
    "    state = states[start_t]\n",
    "    mid_virt_states, mid_virt_rewards = [state], []\n",
    "    for _ in range(200 - start_t):\n",
    "        state, reward, done, _ = env.virtual_step(state, fixed_action)\n",
    "        mid_virt_states.append(state)\n",
    "        mid_virt_rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if np.array(states[start_t:]).shape==np.array(mid_virt_states).shape:\n",
    "        state_diff = np.max(np.linalg.norm(np.array(states[start_t:]) - np.array(mid_virt_states), axis=1))\n",
    "        print('state difference with virual step from the middle:', state_diff)\n",
    "        reward_diff = np.max(np.abs(np.array(rewards[start_t:]) - np.array(mid_virt_rewards)))\n",
    "        print('reward difference with virual step from the middle:', reward_diff)\n",
    "    else:\n",
    "        print('different shapes')\n",
    "    print('time:', time.time() - start_time)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "domain_name = 'InvertedPendulum-v2'\n",
    "env_test(domain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name: Ant-v3\n",
      "state_dim: 111\n",
      "action_dim: 8\n",
      "action_min: [-1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "reward: -2.9851021085135203\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 2.443467234865166e-15\n",
      "reward difference with virual step from the middle: 3.4638958368304884e-14\n",
      "time: 0.3319721221923828\n",
      "\n",
      "\n",
      "domain_name: HalfCheetah-v3\n",
      "state_dim: 17\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "reward: 0.3132147329393614\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 1.9103238021256008e-14\n",
      "reward difference with virual step from the middle: 1.6653345369377348e-15\n",
      "time: 0.07400965690612793\n",
      "\n",
      "\n",
      "domain_name: Hopper-v3\n",
      "state_dim: 11\n",
      "action_dim: 3\n",
      "action_min: [-1. -1. -1.]\n",
      "action_max: [1. 1. 1.]\n",
      "reward: 1.0314365691781826\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 2.2264655880988254e-16\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.028019189834594727\n",
      "\n",
      "\n",
      "domain_name: Humanoid-v3\n",
      "state_dim: 376\n",
      "action_dim: 17\n",
      "action_min: [-0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4\n",
      " -0.4 -0.4 -0.4]\n",
      "action_max: [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "reward: 4.728016428553094\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.00041574501962858323\n",
      "reward difference with virual step from the middle: 23.179188621705663\n",
      "time: 0.13499951362609863\n",
      "\n",
      "\n",
      "domain_name: HumanoidStandup-v2\n",
      "state_dim: 376\n",
      "action_dim: 17\n",
      "action_min: [-0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4 -0.4\n",
      " -0.4 -0.4 -0.4]\n",
      "action_max: [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "reward: 38.627979250308734\n",
      "done: False\n",
      "initial state difference: 195.5551409742709\n",
      "state difference in two attempt: 957.7731395984681\n",
      "reward difference in two attempt: 2.280578554558648\n",
      "state difference with virual step: 1051.818651079651\n",
      "reward difference with virual step: 2.940285388008512\n",
      "state difference with virual step from the middle: 0.0002451204948082124\n",
      "reward difference with virual step from the middle: 3.4747422716918663e-07\n",
      "time: 0.7080020904541016\n",
      "\n",
      "\n",
      "domain_name: InvertedDoublePendulum-v2\n",
      "state_dim: 11\n",
      "action_dim: 1\n",
      "action_min: [-1.]\n",
      "action_max: [1.]\n",
      "reward: 9.144024393841015\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.013996601104736328\n",
      "\n",
      "\n",
      "domain_name: InvertedPendulum-v2\n",
      "state_dim: 4\n",
      "action_dim: 1\n",
      "action_min: [-3.]\n",
      "action_max: [3.]\n",
      "reward: 1.0\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 0.0\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.011984109878540039\n",
      "\n",
      "\n",
      "domain_name: Reacher-v2\n",
      "state_dim: 11\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "reward: -2.328849493335258\n",
      "done: False\n",
      "initial state difference: 0.2174912486417929\n",
      "state difference in two attempt: 6.201337298753301\n",
      "reward difference in two attempt: 0.1041767621468086\n",
      "state difference with virual step: 0.7474030406959685\n",
      "reward difference with virual step: 0.26157974290583974\n",
      "different shapes\n",
      "time: 0.022011280059814453\n",
      "\n",
      "\n",
      "domain_name: Swimmer-v3\n",
      "state_dim: 8\n",
      "action_dim: 2\n",
      "action_min: [-1. -1.]\n",
      "action_max: [1. 1.]\n",
      "reward: 0.0016540545728751794\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 9.321562587279056e-16\n",
      "reward difference with virual step from the middle: 0.0\n",
      "time: 0.07999706268310547\n",
      "\n",
      "\n",
      "domain_name: Walker2d-v3\n",
      "state_dim: 17\n",
      "action_dim: 6\n",
      "action_min: [-1. -1. -1. -1. -1. -1.]\n",
      "action_max: [1. 1. 1. 1. 1. 1.]\n",
      "reward: 1.2164193759808617\n",
      "done: False\n",
      "initial state difference: 0.0\n",
      "state difference in two attempt: 0.0\n",
      "reward difference in two attempt: 0.0\n",
      "state difference with virual step: 0.0\n",
      "reward difference with virual step: 0.0\n",
      "state difference with virual step from the middle: 6.354792872501864e-15\n",
      "reward difference with virual step from the middle: 1.4210854715202004e-14\n",
      "time: 0.14900517463684082\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for domain_name in ['Ant-v3', 'HalfCheetah-v3', 'Hopper-v3', 'Humanoid-v3', \n",
    "                    'HumanoidStandup-v2', 'InvertedDoublePendulum-v2', 'InvertedPendulum-v2',\n",
    "                    'Reacher-v2', 'Swimmer-v3', 'Walker2d-v3']:\n",
    "    env_test(domain_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
